# 创建数据库

~~~sql
create database if not exists myhive;

~~~

- 说明：hive的表存放位置模式是由hive-site.xml当中的一个属性指定的

```xml
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
```

# 创建数据库并指定存储位置

~~~sql
create database myhive2 location '/myhive2';
~~~

# 查看数据库详细信息

```sql 
desc database myhive;

```

![1620831686954](./assets\1620831686954.png)

# 删除数据库

- 删除一个空数据库

~~~sql
drop database myhive;
~~~



- 删除一个数据库及数据库下的表

```sql
drop  database  myhive2  cascade; 
```

# 数据库的表操作

~~~sql
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
   [(col_name data_type [COMMENT col_comment], ...)] 
   [COMMENT table_comment] 
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
   [CLUSTERED BY (col_name, col_name, ...) 
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 
   [ROW FORMAT row_format] 
   [STORED AS file_format] 
   [LOCATION hdfs_path]
~~~

1. CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。

2. EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。

3. LIKE允许用户复制现有的表结构，但是不复制数据。

4. ROW FORMAT DELIMITED 可用来指定行分隔符

5. STORED AS  SEQUENCEFILE|TEXTFILE|RCFILE 来指定该表数据的存储格式，hive中，表的默认存储格式为TextFile。

6、CLUSTERED BY

> 对于每一个表（table）进行分桶(MapReuce中的分区），桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由：
>
> （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。
>
> （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。

7. LOCATION: 指定表在HDFS上的存储位置。

#  Hive建表时候的字段类型

| **分类** | **类型**  | **描述**                                       | **字面量示例**                                               |
| -------- | --------- | ---------------------------------------------- | ------------------------------------------------------------ |
| 原始类型 | BOOLEAN   | true/false                                     | TRUE                                                         |
|          | TINYINT   | 1字节的有符号整数 -128~127                     | 1Y                                                           |
|          | SMALLINT  | 2个字节的有符号整数，-32768~32767              | 1S                                                           |
|          | INT       | 4个字节的带符号整数                            | 1                                                            |
|          | BIGINT    | 8字节带符号整数                                | 1L                                                           |
|          | FLOAT     | 4字节单精度浮点数1.0                           |                                                              |
|          | DOUBLE    | 8字节双精度浮点数                              | 1.0                                                          |
|          | DEICIMAL  | 任意精度的带符号小数                           | 1.0                                                          |
|          | STRING    | 字符串，变长                                   | “a”,’b’                                                      |
|          | VARCHAR   | 变长字符串                                     | “a”,’b’                                                      |
|          | CHAR      | 固定长度字符串                                 | “a”,’b’                                                      |
|          | BINARY    | 字节数组                                       | 无法表示                                                     |
|          | TIMESTAMP | 时间戳，毫秒值精度                             | 122327493795                                                 |
|          | DATE      | 日期                                           | ‘2016-03-29’                                                 |
|          | INTERVAL  | 时间频率间隔                                   |                                                              |
| 复杂类型 | ARRAY     | 有序的的同类型的集合                           | array(1,2)                                                   |
|          | MAP       | key-value,key必须为原始类型，value可以任意类型 | map(‘a’,1,’b’,2)                                             |
|          | STRUCT    | 字段集合,类型可以不同                          | struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) |
|          | UNION     | 在有限取值范围内的一个值                       | create_union(1,’a’,63)                                       |

# 内部表操作

> 未被external修饰的是内部表（managed table）,内部表又称管理表,内部表数据存储的位置由hive.metastore.warehouse.dir参数决定（默认：/user/hive/warehouse），删除内部表会直接删除元数据（metadata）及存储数据，因此内部表不适合和其他工具共享数据。

HIVE创建表

```sql
create database myhive;
use myhive;
create table stu(id int,name string);
insert int stu values (1,"zhangsan");
select * from stu;
```

- 创建表并指定字段之间的分割符

~~~SQL
create  table if not exists stu2(id int ,name string) row format delimited fields terminated by '\t' stored as textfile location '/user/stu2';
~~~

- 根据查询结果创建表

~~~sql
create table stu3 as select * from stu2;
~~~

-  根据已经存在的表结构创建表

~~~sql
create table stu4 like stu2;
~~~

- 查询表类型

  ~~~sql
  desc formatted stu2;
  ~~~

  ![1620834913517](./assets\1620834913517.png)

- 删除表

  ~~~sql
  drop table stu2;
  ~~~

  - 查看数据库和HDFS，发现删除内部表之后，所有的内容全部删除

# 外部表操作



> 在创建表的时候可以指定external关键字创建外部表,外部表对应的文件存储在location指定的hdfs目录下,向该目录添加新文件的同时，该表也会读取到该文件(当然文件格式必须跟表定义的一致)。
>
> 外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive外部表的时候，数据仍然存放在hdfs当中，不会删掉。

- 从 本地加载一张表到HIVE

~~~sql
load data [local] inpath '/export/server/datas/student.txt' overwrite | into table student [partition (partcol1=val1,…)];
~~~

参数:

1.  load data : 表示加载数据
2. local :表示从本地加载数据到HIVE表
3. inpath: 表示加载数据的路径
4. overwrite:表示覆盖表中已有数据，否则表示追加
5. into table:表示加载数据到哪张表
6. student: 表示具体的表
7. partition: 表示上传到指定的分区

# 操作案例

- 分别创建老师与学生表外部表，并向表中加载数据源数据如下

student.txt

~~~
01	赵雷	1990-01-01	男 
02	钱电	1990-12-21	男 
03	孙风	1990-05-20	男 
04	李云	1990-08-06	男 
05	周梅	1991-12-01	女 
06	吴兰	1992-03-01	女 
07	郑竹	1989-07-01	女 
08	王菊	1990-01-20	女 
~~~

teacher.txt

```
01	张三 
02	李四 
03	王五 
```

- 创建学生表

~~~SQL
create external table student (s_id string,s_name string,s_birth string , s_sex string ) row format delimited fields terminated by '\t';
~~~

- 创建老师表

~~~sql
create external table teacher (t_id string,t_name string) row format delimited fields terminated by '\t';
~~~



- 从本地文件系统向表中加载数据

~~~sql
load data local inpath '/export/test/student.txt' into table student;
~~~

- 加载数据并覆盖已有数据

~~~sql
load data local inpath '/export/test/student.txt' overwrite into table student;
~~~



- 从hdfs文件系统向表中加载数据

```sh
cd /export/test/
hadoop fs -mkdir -p /hivedatas/
hadoop fs -put teacher.txt /hivedatas/
load data inpath '/hivedatas/teacher.txt'
```

- **注意删除student表hdfs的数据仍然存在，并且重新创建表之后，表中就直接存在数据了,因为我们的student表使用的是外部表，drop table之后，表当中的数据依然保留在hdfs上面了**

# 复杂类型操作

## array类型

**源数据:**  

说明:name与locations之间制表符分隔，locations中元素之间逗号分隔

- 源数据
- `vim work_locations.txt`

~~~
zhangsan	beijing,shanghai,tianjin,hangzhou
wangwu	changchun,chengdu,wuhan,beijin
~~~

- 建表语句

~~~sql
create table hive_array(name string,work_locations array<string>)
row format delimited fields terminated by '\t'
collection items terminated by ',';

~~~

- 导入数据

~~~sh
load data local inpath '/export/test/work_locations.txt' overwrite into table hive_array;
~~~

- 常用查询

~~~sql
-- 查询所有数据
select * from hive_array;
-- 查询location数组中的第一个元素
select name,work_location[0] location from hive array;
-- 查询location数组中元素个数
select name size(work_locations) location from hive_array;
-- 查询location数组中包含tianjin的信息
select * from hive_array where array_contains(work_locations,'tianjin')
~~~

## map 类型

- 源数据

~~~txt
vim hive_map.txt

1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28
2,lisi,father:mayun#mother:huangyi#brother:guanyu,22
3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29
4,mayun,father:mayongzhen#mother:angelababy,26
~~~

- 建表语句

~~~sql
create table hive_map(
id int, name string, members map<string,string>, age int
)
row format delimited
fields terminated by ','
COLLECTION ITEMS TERMINATED BY '#' 
MAP KEYS TERMINATED BY ':'; 
~~~

- 导入数据

~~~sql
load data local inpath '/export/test/hive_map.txt' overwrite into table hive_map;
~~~

- 常用的查询

~~~sql
SELECT  *FROM  hive_map;

SELECT  id,name,members['father'] father,members['mother'] mother,age 
FROM    hive_map;

SELECT  id,name,size(members) num 
FROM    hive_map;

SELECT  *
FROM    hive_map
WHERE   array_contains(map_keys(members),"brother");

SELECT  id,name,members["brother"] brother 
FROM    hive_map
WHERE   array_contains(map_keys(members),"brother");
~~~

- struct类型

源数据：

说明：字段之间#分割，第二个字段之间冒号分割

```sh
vim hive_struct.txt
```

```sql
192.168.1.1#zhangsan:40
192.168.1.2#lisi:50
192.168.1.3#wangwu:60
192.168.1.4#zhaoliu:70
```

- 建表语句

```sql
CREATE TABLE IF NOT EXISTS hive_struct(
    ip string,
    info struct<name:string,age:int>
)
row format delimited 
fields terminated by '#'
collection items terminated by ':';
```

- 导入数据

~~~sql
load data local inpath '/export/test/hive_struct.txt'
into table hive_struct;
~~~

- 常规的查询

~~~sql
SELECT  *
FROM    hive_struct;


SELECT  ip,info.name
FROM    hive_struct;

~~~

#  内部表和外部表之间的转换

1. 查询表的类型

~~~sql
desc formatted student;
-- Table Type:             MANAGED_TABLE

~~~

2.  修改外部表student为内部表

```sql
ALTER TABLE student SET TBLPROPERTIES('EXTERNAL' = 'FALSE');

```

![1620904367822](./assets\1620904367822.png)

2. 修改内部表student为外部表

~~~sql
ALTER TABLE student SET TBLPROPERTIES('EXTERNAL' = 'TRUE');
~~~

![1620904046288](./assets\1620904046288.png)

3. 查询表的类型

~~~sql
desc formatted student;
-- Table Type:             MANAGED_TABLE
~~~

**注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写！**



# 分区表

> 在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多了。

## 创建分区表

~~~sql
CREATE TABLE score(
    s_id string,
    c_id string,
    s_score int
)
PARTITIONED by (
    month string
)
row format delimited fields terminated by '\t';
~~~

## **创建一个表带多个分区**

```sql
CREATE TABLE IF NOT EXISTS score2(
    s_id string,
    c_id string,
    s_score int
)PARTITIONED by(
    year string,
    month string,
    day string
)row format delimited fields terminated by '\t';
```

## 加载数据到分区表

~~~
cd /export/test
vim score.txt

01	01	80
01	02	90
01	03	99
02	01	70
02	02	60
02	03	80
03	01	80
03	02	80
03	03	80
04	01	50
04	02	30
04	03	20
05	01	76
05	02	87
06	01	31
06	03	34
07	02	89
07	03	98

~~~

~~~sql 
load data local inpath
'/export/test/score.txt'
into table score 
PARTITION(month = '202006');
~~~

## 加载数据到多个分区表中

```sql
load data local inpath
'/export/test/score.txt'
into table score2 
PARTITION(year = '2021', month = '5' ,day = '1');
```

## 查看分区

```sql
show partitions score;
```

# 添加一个分区

~~~sql
alter table score add partition (month = '202005');
~~~

# 同时添加多个分区

~~~sql
alter table score add partition (month = '202004') partition(month = '202003');
~~~

**注意：添加分区之后就可以在hdfs文件系统当中看到表下面多了一个文件夹**

#  删除分区

```sql
alter table score drop partition(month = '202006')
```

# 外部分区表综合练习

> 需求描述：现在有一个文件score.txt文件，存放在集群的这个目录下/scoredatas/month=202006，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除。

1. 数据准备

```sh
hadoop fs -mkdir -p /scoredatas/month=202006
cd /export/test
hadoop fs -put score.txt /scoredatas/month=202006/
```

2. 创建外部分区表，并指定文件数据存放目录

~~~sql
create external table score4(
s_id string,
c_id string,
s_score int
)partitioned by(
month string
)row format delimited fields terminated by '\t' location '/scoredatas';


alter table score4 add partition (month = '202006');
~~~

- **进行表的修复,说白了就是建立我们表与我们数据文件之间的一个关系映射**

~~~sql
msck repair table score4;
elect * from score4;
-- 如下就表示成功
~~~

![1620912229519](./assets\1620912229519.png)

第二种实现方式，上传数据之后手动添加分区即可

```sql
adoop fs -mkdir -p /scoredatas/month=202005
hadoop fs -put score.txt/scoredatas/month=202005
alter table score4 add partition(month='202005');
```

# 分桶表

> 将数据按照指定的字段进行分成多个桶中去，说白了就是将数据按照字段进行划分，可以将数据按照字段划分到多个文件当中去

1. 开启hive分桶功能

~~~sh
set hive.enforce.bucketing=true;
~~~

2. 设置reduce的个数

~~~sql
set mapreduce.job.reduces=3;  
~~~

3. 创建桶表

```sql
CREATE TABLE IF NOT EXISTS course(
	c_id string,
    c_name string,
    t_id string
)clustered by(c_id) into 3
buckets row  format delimited fields terminated by '\t';
```

> 桶表的数据加载，由于桶表的数据加载通过hdfs  dfs  -put文件或者通过load  data均不好使，只能通过insert  overwrite

> 创建普通表，并通过insert  overwrite的方式将普通表的数据通过查询的方式加载到桶表当中去

1. 创建一个普通表

~~~sql
create table course_common(
    c_id string,
    c_name string,
    t_id string
)row format delimited fields terminated by '\t';
~~~

2. 加载数据到普通表

~~~sh
vim course.txt

01	语文	02
02	数学	01
03	英语	03

~~~



~~~sql
load data local inpath
'/export/test/course.txt'
into table course_common;
~~~

3. 通过insert  overwrite给桶表中加载数据

~~~sql
insert overwrite table
course
SELECT * FROM 
course_common
cluster by (c_id);
~~~

# 修改表

## 表重命名

基本语法

```sql
alter  table  old_table_name  rename  to  new_table_name;
```

~~~sql
-- 把表score4 修改成score5
ALTER TABLE score4 RENAME TO score5;
~~~

2. 增加/修改列信息

~~~sql
-- 1 查询表结构
desc score5;

-- 2. 添加列
alter table score5 add columns(
    mycol string,
    mysco string
);

-- 3. 查询表结构
desc score5;
-- 4. 更新列
alter table score5 change column mysco mysconew int;
-- 5:查询表结构
desc score5
~~~

## 删除表

~~~sql
drop table score5;
~~~

清空表数据

~~~sql
truncate table score6;
~~~

# hive表中加载数据

## 直接往分区表中加载数据

~~~sql
CREATE TABLE IF NOT EXISTS score3
LIKE score;
insert into table score3 partition(month = '202008') values('001','002','100');
~~~

## 通过查询方式加载数据

```sql
create table score4 like score;
insert overwrite table score4 partition(month = '202006') select s_id,c_id,s_score from score;
```

## 通过查询插入数据

```sql
load data local inpath
'/export/test/score.txt'
overwrite into table 
score
partition(month='202006');
```

# 多插入模式

- 给score表加载数据

```sql
load data local inpath
'/export/test/score.txt'
overwrite into table 
score
partition(month='202006');


```

- 创建第一部分表：

```sql
create table score_first(
    s_id string,
    c_id string
)partitioned by(
    month string
)row format delimited 
fields terminated
by '\t';
```

- 创建第二部分表：

```sql
create table score_second(
    s_id string,
    c_id string
)partitioned by(
    month string
)row format delimited 
fields terminated
by '\t';
```

- 分别给第一部分与第二部分表加载数据

```sql
FROM score 
INSERT OVERWRITE TABLE score_first
partition(month='202006')
SELECT
    s_id,c_id
INSERT OVERWRITE TABLE score_second
partition(month='202006')
SELECT
    c_id,s_score;

```

## 查询语句中创建表并加载数据（as  select）

- 将查询的结果保存到一张表当中去

```sql
create table score5 as SELECT  * FROM    score;
```

# 创建表时通过location指定加载数据路径

-  创建表，并指定在hdfs上的位置

```sql
create external table score6(
    s_id string,
    c_id string,
    s_score int
)row format delimited 
fields terminated by '\t'
location '/myscore6';
```

- 上传数据到hdfs上

~~~sh
hadoop fs -mkdir -p /myscore6
hadoop fs -put score.txt/myscore6;
~~~

-  查询数据

```sql
SELECT * FROM score6;
```

# export导出与import **导入** hive表数据（内部表操作）

```sql
-- 创建结构和teacher一样的teacher2 表
create table teacher2 like teacher;

-- 把teacher表数据导出到hdfs
export table teacher to '/export/test/teacher.txt';
-- 把数据导入到teacher2
import table teacher2 FROM '/export/test/teacher.txt';
-- 查看teacher2是否有数据
SELECT   * FROM    teacher2;
```

# hive表中的数据导出

## insert导出

- 将查询的结果导出到本地

~~~sql
insert overwrite local directory 
'/export/test/exporthive'
SELECT * FROM score;
~~~

- 将查询的结果格式化导出到本地

```sql
insert overwrite local directory 
'/export/test/exporthive'
row format delimited fields terminated by '\t'
collection items terminated by '#'
SELECT * FROM student;
```

- 将查询的结果导出到HDFS上(没有local)

```sql
insert overwrite  directory 
'/export/test/exporthive'
row format delimited fields terminated by '\t'
collection items terminated by '#'
SELECT * FROM score;
```

## hive shell 命令导出

```sh
hive -e "select * from myhive.score;" > /export/server/exporthive/score1.txt
```

## export导出到HDFS上

~~~sql
export table score to '/export/exporthive/score';
~~~

# 也可以通过Sqoop导出

